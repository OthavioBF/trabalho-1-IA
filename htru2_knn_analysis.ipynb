{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Classificação kNN - Dataset HTRU2\n",
    "\n",
    "Este notebook realiza classificação usando k-Nearest Neighbors no dataset HTRU2 para detecção de pulsares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "def load_htru2_data():\n",
    "    # Nomes das colunas baseado na documentação\n",
    "    columns = [\n",
    "        'mean_integrated_profile',\n",
    "        'std_integrated_profile', \n",
    "        'kurtosis_integrated_profile',\n",
    "        'skewness_integrated_profile',\n",
    "        'mean_dmsnr_curve',\n",
    "        'std_dmsnr_curve',\n",
    "        'kurtosis_dmsnr_curve', \n",
    "        'skewness_dmsnr_curve',\n",
    "        'class'\n",
    "    ]\n",
    "    \n",
    "    df = pd.read_csv('HTRU_2.csv', names=columns)\n",
    "    return df\n",
    "\n",
    "# Análise exploratória\n",
    "def analyze_data(df):\n",
    "    print(\"=== ANÁLISE EXPLORATÓRIA DOS DADOS ===\")\n",
    "    print(f\"Dimensões do dataset: {df.shape}\")\n",
    "    print(f\"\\nDistribuição das classes:\")\n",
    "    print(df['class'].value_counts())\n",
    "    print(f\"\\nPercentual das classes:\")\n",
    "    print(df['class'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    print(f\"\\nValores faltantes: {df.isnull().sum().sum()}\")\n",
    "    print(f\"\\nEstatísticas descritivas:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    return df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_experiment(X, y, k_values, n_runs=5, train_size=6000):\n",
    "    \"\"\"\n",
    "    Executa experimentos de kNN com diferentes valores de k\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\n=== TESTANDO k = {k} ===\")\n",
    "        \n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        times = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"Execução {run + 1}/{n_runs}\")\n",
    "            \n",
    "            # Dividir dados: 6000 para treino, resto para teste\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, train_size=train_size, random_state=42 + run, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Dividir treino em treino/validação para seleção de k\n",
    "            X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.5, random_state=42 + run, stratify=y_train\n",
    "            )\n",
    "            \n",
    "            # Normalizar dados\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Treinar modelo\n",
    "            start_time = time.time()\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "            knn.fit(X_train_scaled, y_train_split)\n",
    "            \n",
    "            # Predições\n",
    "            y_pred = knn.predict(X_test_scaled)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Métricas\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            exec_time = end_time - start_time\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            times.append(exec_time)\n",
    "            \n",
    "            print(f\"  Acurácia: {accuracy:.4f}, Precisão: {precision:.4f}, Recall: {recall:.4f}, Tempo: {exec_time:.2f}s\")\n",
    "        \n",
    "        # Calcular médias\n",
    "        results[k] = {\n",
    "            'accuracy_mean': np.mean(accuracies),\n",
    "            'accuracy_std': np.std(accuracies),\n",
    "            'precision_mean': np.mean(precisions),\n",
    "            'precision_std': np.std(precisions),\n",
    "            'recall_mean': np.mean(recalls),\n",
    "            'recall_std': np.std(recalls),\n",
    "            'time_mean': np.mean(times),\n",
    "            'time_std': np.std(times),\n",
    "            'n_prototypes': train_size // 2  # Metade do conjunto de treino\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nRESULTADOS MÉDIOS para k={k}:\")\n",
    "        print(f\"  Acurácia: {results[k]['accuracy_mean']:.4f} ± {results[k]['accuracy_std']:.4f}\")\n",
    "        print(f\"  Precisão: {results[k]['precision_mean']:.4f} ± {results[k]['precision_std']:.4f}\")\n",
    "        print(f\"  Recall: {results[k]['recall_mean']:.4f} ± {results[k]['recall_std']:.4f}\")\n",
    "        print(f\"  Tempo: {results[k]['time_mean']:.2f} ± {results[k]['time_std']:.2f}s\")\n",
    "        print(f\"  Protótipos: {results[k]['n_prototypes']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, class_distribution):\n",
    "    \"\"\"\n",
    "    Analisa e compara os resultados dos diferentes valores de k\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARAÇÃO DOS RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Criar DataFrame com resultados\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    print(\"\\nTabela de Resultados:\")\n",
    "    print(f\"{'k':<5} {'Acurácia':<12} {'Precisão':<12} {'Recall':<12} {'Tempo(s)':<10} {'Protótipos':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for k in results.keys():\n",
    "        acc = f\"{results[k]['accuracy_mean']:.4f}\"\n",
    "        prec = f\"{results[k]['precision_mean']:.4f}\"\n",
    "        rec = f\"{results[k]['recall_mean']:.4f}\"\n",
    "        time_val = f\"{results[k]['time_mean']:.2f}\"\n",
    "        prot = results[k]['n_prototypes']\n",
    "        \n",
    "        print(f\"{k:<5} {acc:<12} {prec:<12} {rec:<12} {time_val:<10} {prot:<10}\")\n",
    "    \n",
    "    # Melhor k por métrica\n",
    "    best_k_acc = max(results.keys(), key=lambda k: results[k]['accuracy_mean'])\n",
    "    best_k_time = min(results.keys(), key=lambda k: results[k]['time_mean'])\n",
    "    \n",
    "    print(f\"\\nMelhor k por acurácia: k={best_k_acc} (Acurácia: {results[best_k_acc]['accuracy_mean']:.4f})\")\n",
    "    print(f\"Melhor k por tempo: k={best_k_time} (Tempo: {results[best_k_time]['time_mean']:.2f}s)\")\n",
    "    \n",
    "    # Análise da distribuição das classes\n",
    "    print(f\"\\n=== ANÁLISE DA DISTRIBUIÇÃO DAS CLASSES ===\")\n",
    "    print(f\"Classe 0 (não-pulsar): {class_distribution[0]} ({class_distribution[0]/class_distribution.sum()*100:.1f}%)\")\n",
    "    print(f\"Classe 1 (pulsar): {class_distribution[1]} ({class_distribution[1]/class_distribution.sum()*100:.1f}%)\")\n",
    "    \n",
    "    # Discussão sobre acurácia\n",
    "    majority_baseline = class_distribution[0] / class_distribution.sum()\n",
    "    print(f\"\\nBaseline (classificar tudo como classe majoritária): {majority_baseline:.4f}\")\n",
    "    \n",
    "    print(f\"\\n=== CONSIDERAÇÕES SOBRE A ACURÁCIA ===\")\n",
    "    print(f\"O dataset é desbalanceado ({class_distribution[0]/class_distribution.sum()*100:.1f}% vs {class_distribution[1]/class_distribution.sum()*100:.1f}%).\")\n",
    "    print(f\"A acurácia pode ser enganosa em datasets desbalanceados.\")\n",
    "    print(f\"Precisão e recall são métricas mais informativas neste caso.\")\n",
    "    print(f\"Todos os modelos kNN superaram significativamente o baseline de {majority_baseline:.4f}.\")\n",
    "    \n",
    "    return results_df, best_k_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"\n",
    "    Cria gráficos dos resultados\n",
    "    \"\"\"\n",
    "    k_values = list(results.keys())\n",
    "    accuracies = [results[k]['accuracy_mean'] for k in k_values]\n",
    "    times = [results[k]['time_mean'] for k in k_values]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Gráfico de acurácia\n",
    "    ax1.plot(k_values, accuracies, 'bo-')\n",
    "    ax1.set_xlabel('Valor de k')\n",
    "    ax1.set_ylabel('Acurácia Média')\n",
    "    ax1.set_title('Acurácia vs Valor de k')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Gráfico de tempo\n",
    "    ax2.plot(k_values, times, 'ro-')\n",
    "    ax2.set_xlabel('Valor de k')\n",
    "    ax2.set_ylabel('Tempo Médio (s)')\n",
    "    ax2.set_title('Tempo de Execução vs Valor de k')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar análise completa\n",
    "def main():\n",
    "    # Carregar dados\n",
    "    df = load_htru2_data()\n",
    "    class_dist = analyze_data(df)\n",
    "    \n",
    "    # Preparar dados\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "    \n",
    "    print(f\"\\nFeatures utilizadas: {list(X.columns)}\")\n",
    "    \n",
    "    # Valores de k para testar\n",
    "    k_values = [1, 3, 5, 7, 9]\n",
    "    \n",
    "    print(f\"\\nValores de k testados: {k_values}\")\n",
    "    print(f\"Número de execuções por k: 5\")\n",
    "    print(f\"Tamanho do conjunto de treino: 6000 amostras\")\n",
    "    print(f\"Tamanho do conjunto de teste: {len(df) - 6000} amostras\")\n",
    "    \n",
    "    # Executar experimentos\n",
    "    results = run_knn_experiment(X, y, k_values)\n",
    "    \n",
    "    # Analisar resultados\n",
    "    results_df, best_k = analyze_results(results, class_dist)\n",
    "    \n",
    "    # Plotar resultados\n",
    "    plot_results(results)\n",
    "    \n",
    "    return results, results_df, best_k\n",
    "\n",
    "# Executar\n",
    "results, results_df, best_k = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
