{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Regressão Linear - Communities and Crime Dataset\n",
    "\n",
    "Este notebook realiza análise de regressão linear para predizer a variável ViolentCrimesPerPop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "def load_data():\n",
    "    # Definir nomes das colunas baseado no arquivo .names\n",
    "    columns = [\n",
    "        'state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize',\n",
    "        'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
    "        'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf',\n",
    "        'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc',\n",
    "        'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap',\n",
    "        'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore',\n",
    "        'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu',\n",
    "        'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv',\n",
    "        'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par',\n",
    "        'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
    "        'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5',\n",
    "        'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell',\n",
    "        'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous',\n",
    "        'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR',\n",
    "        'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
    "        'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart',\n",
    "        'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent',\n",
    "        'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters',\n",
    "        'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85',\n",
    "        'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
    "        'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic',\n",
    "        'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp',\n",
    "        'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
    "        'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg',\n",
    "        'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop',\n",
    "        'ViolentCrimesPerPop'\n",
    "    ]\n",
    "    \n",
    "    # Carregar dados\n",
    "    df = pd.read_csv('communities.data', names=columns, na_values='?')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(df):\n",
    "    print(\"=== ANÁLISE EXPLORATÓRIA DOS DADOS ===\")\n",
    "    print(f\"Dimensões do dataset: {df.shape}\")\n",
    "    print(f\"\\nVariável alvo: ViolentCrimesPerPop\")\n",
    "    print(f\"Valores únicos na variável alvo: {df['ViolentCrimesPerPop'].nunique()}\")\n",
    "    print(f\"Valores faltantes na variável alvo: {df['ViolentCrimesPerPop'].isnull().sum()}\")\n",
    "    \n",
    "    # Análise de valores faltantes\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Coluna': missing_data.index,\n",
    "        'Valores_Faltantes': missing_data.values,\n",
    "        'Percentual': missing_percent.values\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Percentual', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== VALORES FALTANTES ===\")\n",
    "    print(f\"Total de colunas com valores faltantes: {len(missing_df)}\")\n",
    "    print(\"\\nTop 10 colunas com mais valores faltantes:\")\n",
    "    print(missing_df.head(10))\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    print(\"\\n=== PRÉ-PROCESSAMENTO DOS DADOS ===\")\n",
    "    \n",
    "    # Remover colunas não preditivas\n",
    "    non_predictive = ['state', 'county', 'community', 'communityname', 'fold']\n",
    "    df_clean = df.drop(columns=non_predictive)\n",
    "    \n",
    "    # Separar features e target\n",
    "    X = df_clean.drop('ViolentCrimesPerPop', axis=1)\n",
    "    y = df_clean['ViolentCrimesPerPop']\n",
    "    \n",
    "    # Remover linhas onde a variável alvo é nula\n",
    "    mask = ~y.isnull()\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"Dados após remoção de valores nulos na variável alvo: {X.shape}\")\n",
    "    \n",
    "    # Análise de colunas com muitos valores faltantes\n",
    "    missing_threshold = 0.5  # 50%\n",
    "    missing_cols = X.columns[X.isnull().mean() > missing_threshold].tolist()\n",
    "    \n",
    "    print(f\"\\nColunas com mais de {missing_threshold*100}% de valores faltantes:\")\n",
    "    for col in missing_cols:\n",
    "        pct = X[col].isnull().mean() * 100\n",
    "        print(f\"  {col}: {pct:.1f}%\")\n",
    "    \n",
    "    # Estratégia de tratamento de valores faltantes\n",
    "    print(f\"\\n=== ESTRATÉGIAS DE TRATAMENTO ===\")\n",
    "    print(\"1. Colunas com >50% de valores faltantes: Removidas\")\n",
    "    print(\"2. Demais colunas: Imputação pela mediana\")\n",
    "    \n",
    "    # Remover colunas com muitos valores faltantes\n",
    "    X_processed = X.drop(columns=missing_cols)\n",
    "    \n",
    "    # Imputar valores faltantes restantes com a mediana\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_processed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_processed),\n",
    "        columns=X_processed.columns,\n",
    "        index=X_processed.index\n",
    "    )\n",
    "    \n",
    "    print(f\"Dimensões finais dos dados: {X_processed.shape}\")\n",
    "    print(f\"Valores faltantes restantes: {X_processed.isnull().sum().sum()}\")\n",
    "    \n",
    "    return X_processed, y, missing_cols, imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y):\n",
    "    print(\"\\n=== DIVISÃO DOS DADOS E TREINAMENTO ===\")\n",
    "    \n",
    "    # Divisão treino/teste (70/30)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Tamanho do conjunto de treino: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Tamanho do conjunto de teste: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Treinar modelo de regressão linear\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predições\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"\\n=== RESULTADOS DO MODELO ===\")\n",
    "    print(f\"TREINO - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"TESTE  - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
    "    print(f\"R² Score (treino): {model.score(X_train, y_train):.4f}\")\n",
    "    print(f\"R² Score (teste): {model.score(X_test, y_test):.4f}\")\n",
    "    \n",
    "    return model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_analysis(model, X):\n",
    "    print(f\"\\n=== ANÁLISE DE IMPORTÂNCIA DAS FEATURES ===\")\n",
    "    \n",
    "    # Coeficientes do modelo\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': model.coef_,\n",
    "        'Abs_Coefficient': np.abs(model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features mais importantes (por magnitude do coeficiente):\")\n",
    "    print(feature_importance.head(10)[['Feature', 'Coefficient']])\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar análise completa\n",
    "df = load_data()\n",
    "missing_analysis = analyze_data(df)\n",
    "X, y, removed_cols, imputer = preprocess_data(df)\n",
    "model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred = train_model(X, y)\n",
    "importance = feature_importance_analysis(model, X)\n",
    "\n",
    "print(f\"\\n=== CONSIDERAÇÕES FINAIS ===\")\n",
    "print(f\"1. Foram removidas {len(removed_cols)} colunas com >50% de valores faltantes\")\n",
    "print(f\"2. Valores faltantes restantes foram imputados pela mediana\")\n",
    "print(f\"3. O modelo final usa {X.shape[1]} features para predizer ViolentCrimesPerPop\")\n",
    "print(f\"4. Colunas removidas: {removed_cols[:5]}...\" if len(removed_cols) > 5 else f\"4. Colunas removidas: {removed_cols}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
